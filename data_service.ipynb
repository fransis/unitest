import os
import sys
from glob import glob
import json
from edge import edge_socket
import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle
from datetime import datetime


class data_service_server:
    def __init__(self, dataset: str):
        self.dataset = dataset
        self.init_message_handler()
        self.exit = False

        self.load_config()
        self.config_dirty = False
        
        self.csv_file = dict()
        self.csv_dirty = set()

        self.data_file = dict()
        self.data_dirty = set()

        self.model_file = dict()

        self.socket = edge_socket().bind('data_service', self.dataset)
        self.print_log(f'waiting for requests')

    def print_log(self, msg):
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: [data_service][{self.dataset}] {msg}")

    def server(self):
        while not self.exit:
            message = self.socket.recv()
            self.process_message(message)

    def process_message(self, tokens: tuple):
        if len(tokens) > 0:
            message_handler = self.message_handler
            while isinstance(message_handler, dict):
                if tokens[0] in message_handler:
                    message_handler = message_handler[tokens[0]]
                    tokens = tokens[1:]
                else:
                    message_handler = None
                    self.socket.send((False, f'unknown token: {tokens[0]}'))
                    break
            if message_handler is not None:
                message_handler(self, tokens)
        else:
            self.socket.send((False, 'command needed'))

    def save(self):
        self.save_config()
        self.save_csv()
        self.save_data()

    def load_config(self):
        with open(f'{self.dataset}/data_service.json', 'r') as f:
            self.config = json.load(f)
        self.field_name = np.array([data_service_server.convert_field_name(name) for name in self.config['field_names']])
        self.field_visible = np.array([name[0] != '~' for name in self.config['field_names']])
        self.field_cpbase = np.array([name[-1] == '!' for name in self.config['field_names']])
        self.field_index = {name: i for i, name in enumerate(self.field_name)}
        self.field_count = self.field_name.shape[0]
        self.field_visible_count = self.field_visible.sum()

        if 'scale' in self.config:
            self.scale = np.expand_dims(np.array(self.config['scale'], dtype=np.float32), axis=0)
        else:
            self.scale = np.full((1, self.field_visible_count), 1., dtype=np.float32)
        if 'offset' in self.config:
            self.offset = np.expand_dims(np.array(self.config['offset'], dtype=np.float32), axis=0)
        else:
            self.offset = np.zeros((1, self.field_visible_count,), dtype=np.float32)

    def save_config(self):
        if self.config_dirty:
            with open(f'{self.dataset}/data_service.json', 'w') as f:
                json.dump(self.config, f, indent=4)
            self.config_dirty = True

    def save_csv(self):
        path = f'{self.dataset}/csv'
        if not os.path.exists(path):
            os.mkdir(path)
        for date in self.csv_dirty:
            lines = [f'{dt:010d}, {op:.2f}, {hp:.2f}, {lp:.2f}, {cp:.2f}, {vol:.2f}\n' for dt, op, hp, lp, cp, vol in self.csv_file[date]]
            with open(f'{path}/{date:06d}.csv', 'wt') as f:
                f.writelines(lines)
        self.csv_dirty.clear()

    def load_csv(self, date: int):
        if date not in self.csv_file:
            fn = f'{self.dataset}/csv/{date:06d}.csv'
            csv = []
            if os.path.exists(fn):
                with open(fn, 'rt') as f:
                    lines = [l for l in [l.strip() for l in f.readlines()] if len(l) > 0]
                lines = sorted(lines)
                if len(lines) > 0:
                    csv = [(lambda x: (int(x[0]), float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5])))(l.split(',')) for l in lines]
            self.csv_file[date] = csv

    def get_csv_last_datetime(self):
        dates = [int(fn.replace('\\', '/').split('/')[-1].split('.')[0]) for fn in sorted(glob(f'{self.dataset}/csv/??????.csv'))]
        if len(dates) > 0:
            last_date = dates[-1]
            self.load_csv(last_date)
        if len(self.csv_file) > 0:
            return (True, self.csv_file[last_date][-1][0])
        else:
            return (False, 'no data')

    def get_csv_first_datetime(self):
        dates = [int(fn.replace('\\', '/').split('/')[-1].split('.')[0]) for fn in sorted(glob(f'{self.dataset}/csv/??????.csv'))]
        if len(dates) > 0:
            first_date = dates[0]
            self.load_csv(first_date)
        if len(self.csv_file) > 0:
            return (True, self.csv_file[first_date][0][0])
        else:
            return (False, 'no data')
        
    def append_csv(self, datetime: int, open: float, high: float, low: float, close: float, vol: float):
        date, time = divmod(datetime, 10000)
        self.load_csv(date)
        csv = self.csv_file[date]
        if len(csv) == 0 or csv[-1][0] < datetime:
            csv.append([datetime, open, high, low, close, vol])
        elif csv[-1][0] == datetime:
            csv[-1] = (datetime, open, high, low, close, vol)
        else:
            return (False, 'datetime should be same or bigger')
        self.csv_dirty.add(date)
        return self.append_data(datetime)
        
    def get_csv(self, date: int):
        self.load_csv(date)
        csv = self.csv_file[date]
        return (True, csv)
        
    def save_data(self):
        path = f'{self.dataset}/data'
        if not os.path.exists(path):
            os.mkdir(path)
        for date in self.data_dirty:
            np.savez(f'{path}/{date:06d}.npz', **self.data_file[date])
        self.data_dirty.clear()
    
    def load_data(self, date: int):
        if date not in self.data_file:
            fn = f'{self.dataset}/data/{date:06d}.npz'
            if os.path.exists(fn):
                npz = np.load(fn)
                self.data_file[date] = {
                    'start_datetime': int(npz['start_datetime']),
                    'array': npz['array'],
                }
            else:
                self.data_file[date] = {
                    'start_datetime': 0,
                    'array': np.array((0, self.field_count), dtype=np.float32)
                }

    def append_data(self, datetime: int):
        date, time = divmod(datetime, 10000)
        self.load_data(date)
        npz = self.data_file[date]
        start_datetime = npz['start_datetime']
        self.d_ar = npz['array']
        if start_datetime == 0 or self.d_ar.shape[0] == 0:
            self.d_idx = 0
            self.d_ar = np.full((1, self.field_count), np.nan, dtype=np.float32)
            npz['array'] = self.d_ar
            npz['start_datetime'] = datetime
        else:
            self.d_idx = data_service_server.time2mins(time) - data_service_server.time2mins(start_datetime % 10000)
            last_idx = self.d_ar.shape[0] - 1
            if self.d_idx < last_idx:
                return (False, 'datetime should be same or bigger')
            elif self.d_idx == last_idx:
                self.d_ar[self.d_idx, :] = np.nan
            else:
                ar = np.full((self.d_idx - last_idx, self.field_count), np.nan, dtype=np.float32)
                self.d_ar = np.concatenate((self.d_ar, ar), axis=0)
                npz['array'] = self.d_ar
        self.build_data(self.csv_file[date][-1])
        self.data_dirty.add(date)
        return (True, )
    
    def build_data(self, rec: list):
        f_close = self.field_index['close']
        self.d_update(self.field_index['open'], rec[1])
        self.d_update(self.field_index['high'], rec[2])
        self.d_update(self.field_index['low'], rec[3])
        self.d_update(f_close, rec[4])
        self.d_update(self.field_index['vol'], rec[5])

        self.d_update(self.field_index['diff:close'], self.d_diff(self.field_index['close']))
        self.d_update(self.field_index['diff:high'], self.d_diff(self.field_index['high']))
        self.d_update(self.field_index['diff:low'], self.d_diff(self.field_index['low']))
        self.d_update(self.field_index['tp'], \
                      (self.d_value(self.field_index['high']) + self.d_value(self.field_index['low']) + self.d_value(self.field_index['close'])) / 3)

        self.d_update(self.field_index['ma(5):close'], self.d_ma(f_close, 5))
        self.d_update(self.field_index['ma(10):close'], self.d_ma(f_close, 10))
        self.d_update(self.field_index['ma(20):close'], self.d_ma(f_close, 20))
        self.d_update_macd(3, 10, 3)
        self.d_update_macd(5, 15, 5)
        self.d_update_sto(5, 3, 3)
        self.d_update_sto(9, 3, 3)
        self.d_update_rsi(14)
        self.d_update_bol(20)
        self.d_update_cci(20)
        self.d_update_adx(14)
        self.d_update_disp(5)
        self.d_update_disp(20)
        self.d_update_psy(10)
        self.d_update_obv(9)
        self.d_update_mfi(14)
        self.d_update_vo(5, 10)
        self.d_update_vr(14)
        self.d_update_sar(0.02, 0.2)
        self.d_update_trix(14)
        self.d_update_sonar(9, 6, 5)

    def rebuild_data(self):
        csv_path = f'{self.dataset}/csv'
        if not os.path.exists(csv_path):
            return (False, 'dataset not found')
        data_path = f'{self.dataset}/data'
        self.print_log(f'clear {data_path}')
        if not os.path.exists(data_path):
            os.mkdir(data_path)
        for fn in glob(f'{data_path}/*.npz'):
            os.remove(fn)
        self.data_file = dict()
        self.data_dirty = set()
        self.print_log(f'rebuilding {data_path}')
        for fn in tqdm(sorted(glob(f'{csv_path}/*.csv'))):
            date = int(fn.replace('\\', '/').split('/')[-1].split('.')[0])
            self.load_csv(date)
            csv = self.csv_file[date]
            start_datetime = csv[0][0]
            end_datetime = csv[-1][0]
            start_time = start_datetime % 10000
            start_mins = data_service_server.time2mins(start_time)
            end_mins = data_service_server.time2mins(end_datetime % 10000)
            rec_count = end_mins - start_mins + 1
            self.d_ar = np.full((rec_count, self.field_count), np.nan, dtype=np.float32)
            self.data_file[date] = {
                'start_datetime': start_datetime,
                'array': self.d_ar
            }
            for r in csv:
                self.d_idx = data_service_server.time2mins(r[0] % 10000) - start_mins
                self.build_data(r)
            self.data_dirty.add(date)
        self.print_log(f'done')
        return (True, )
    
    def get_train_data(self, input_size: int, aheads: list=[], start_date: int=0, end_date: int=0):
        dates = [int(fn.replace('\\', '/').split('/')[-1].split('.')[0]) for fn in sorted(glob(f'{self.dataset}/data/??????.npz'))]
        dates = [date for date in dates if (start_date == 0 or date >= start_date) and (end_date == 0 or date <= end_date)]
        for date in dates:
            self.load_data(date)
        dates = [date for date in self.data_file.keys() if (start_date == 0 or date >= start_date) and (end_date == 0 or date <= end_date)]
        f_close = self.field_index['close']
        Xs = []
        Ys = []
        for date in dates:
            array = self.data_file[date]['array']
            close = array[:, f_close]
            # make y
            X = array.copy()
            X[:, self.field_cpbase] -= np.expand_dims(close, axis=1)
            X = X[:, self.field_visible]
            X -= self.offset
            X /= self.scale
            X = np.lib.stride_tricks.sliding_window_view(X, (input_size, X.shape[1])).reshape((X.shape[0] - input_size + 1, X.shape[1] * input_size))
            mask = ~np.isnan(X).any(axis=1)
            X = X[mask]
            Xs.append(X)
            # make y
            if len(aheads) > 0:
                cp = data_service_server.fillna(close)
                Y = np.full((array.shape[0], len(aheads)), np.nan, dtype=np.float32)
                for i, ahead in enumerate(aheads):
                    y = np.concatenate((cp[ahead:], np.full((ahead,), np.nan, dtype=np.float32)), axis=0)
                    Y[:, i] = data_service_server.ffill(y)
                Y -= np.expand_dims(close, axis=1)
                Ys.append(Y[input_size - 1:][mask])
        X = np.concatenate(Xs, axis=0)
        Y = np.concatenate(Ys, axis=0) if len(aheads) > 0 else None
        return X, Y

    def get_test_data(self, input_size: int, start_date: int=0, end_date: int=0):
        dates = [int(fn.replace('\\', '/').split('/')[-1].split('.')[0]) for fn in sorted(glob(f'{self.dataset}/data/??????.npz'))]
        dates = [date for date in dates if (start_date == 0 or date >= start_date) and (end_date == 0 or date <= end_date)]
        for date in dates:
            self.load_data(date)
        dates = [date for date in self.data_file.keys() if (start_date == 0 or date >= start_date) and (end_date == 0 or date <= end_date)]
        results = []
        f_close = self.field_index['close']
        for date in dates:
            # make X, cp
            npz = self.data_file[date]
            X = npz['array']
            start_datetime = npz['start_datetime']
            cp = X[:, f_close]
            X = X.copy()
            # cp_base
            X[:, self.field_cpbase] -= np.expand_dims(X[:, self.field_index['close']], axis=1)
            # visible
            X = X[:, self.field_visible]
            # normalize
            X -= self.offset
            X /= self.scale
            # input_size
            X = np.lib.stride_tricks.sliding_window_view(X, (input_size, X.shape[1])).reshape((X.shape[0] - input_size + 1, X.shape[1] * input_size))
            cp = cp[input_size - 1:]
            # append result
            results.append((start_datetime, X, cp))
        return results

    def get_data(self, input_size: int, datetime: int):
        date, time = divmod(datetime, 10000)
        f_close = self.field_index['close']
        # make X, cp
        self.load_data(date)
        npz = self.data_file[date]
        ar = npz['array']
        start_datetime = npz['start_datetime']
        idx = data_service_server.time2mins(time) - data_service_server.time2mins(start_datetime % 10000)
        e_idx = idx + 1
        s_idx = max(e_idx - input_size, 0)
        idx_count = e_idx - s_idx
        if idx_count < input_size:
            X = np.full((input_size, ar.shape[1]), np.nan, dtype=ar.dtype)
            y = np.full((1, ar.shape[1]), np.nan, dtype=ar.dtype)
        else:
            X = ar[s_idx:e_idx].copy()
            y = ar[idx:idx+1, f_close]
        # cp_base
        X[:, self.field_cpbase] -= np.expand_dims(X[:, self.field_index['close']], axis=1)
        # visible
        X = X[:, self.field_visible]
        # normalize
        X -= self.offset
        X /= self.scale
        # input_size
        if X.shape[0] == 0:
            X = X.reshape((0, X.shape[1] * input_size))
        else:
            X = X.reshape((1, X.shape[1] * input_size))
        # append result
        return X, y

    @staticmethod
    def time2mins(time: int):
        return (r := divmod(time, 100))[0] * 60 + r[1]

    @staticmethod
    def convert_field_name(field_name: str):
        if field_name[0] == '~':
            field_name = field_name[1:]
        if field_name[-1] == '!':
            field_name = field_name[:-1]
        return field_name

    @staticmethod
    def ffill(arr: np.ndarray):
        mask = np.isnan(arr)
        idx = np.where(~mask, np.arange(mask.size), 0)
        np.maximum.accumulate(idx, out=idx)
        return arr[idx]

    @staticmethod
    def bfill(arr: np.ndarray):
        return data_service_server.ffill(arr[::-1])[::-1]

    @staticmethod
    def fillna(arr: np.ndarray):
        return data_service_server.ffill(data_service_server.bfill(arr))


    # data manager
    def d_window(self, f_source :int, period :int):
        return self.d_ar[max(0, self.d_idx + 1 - period):self.d_idx + 1, f_source]

    def d_value(self, f_source :int):
        return self.d_ar[self.d_idx, f_source]
    
    def d_prev(self, f_source :int, period :int=1):
        return self.d_ar[self.d_idx - period, f_source] if self.d_idx - period >= 0 else np.nan
    
    def d_diff(self, f_source :int):
        return self.d_value(f_source) - self.d_prev(f_source)

    def d_ma(self, f_source :int, period :int):
        ar = self.d_window(f_source, period)
        return ar.mean() if ar.shape[0] == period else np.nan

    def d_std(self, f_source :int, period :int):
        ar = self.d_window(f_source, period)
        return ar.std() if ar.shape[0] == period else np.nan

    def d_min(self, f_source :int, period :int):
        ar = self.d_window(f_source, period)
        return ar.min() if ar.shape[0] == period else np.nan
        
    def d_max(self, f_source :int, period :int):
        ar = self.d_window(f_source, period)
        return ar.max() if ar.shape[0] == period else np.nan

    def d_update(self, f_target :int, value: float):
        self.d_ar[self.d_idx, f_target] = value

    def d_update_ema(self, f_target :int, f_source :int, period :int):
        if self.d_idx == 0 or np.isnan(self.d_prev(f_target)):
            ema = self.d_value(f_source)
        else:
            alpha = 2 / (period + 1)
            ema = alpha * self.d_value(f_source) + (1 - alpha) * self.d_prev(f_target)
        self.d_update(f_target, ema)

    def d_update_macd(self, fast_period :int, slow_period :int, sig_period :int):
        key = f'macd({fast_period},{slow_period},{sig_period})'
        f_line = self.field_index[key + ':line']
        f_sig = self.field_index[key + ':sig']
        f_hist = self.field_index[key + ':hist']
        f_close = self.field_index['close']
        f_fast = self.field_index[f'ema({fast_period}):close']
        self.d_update_ema(f_fast, f_close, fast_period)
        f_slow = self.field_index[f'ema({slow_period}):close']
        self.d_update_ema(f_slow, f_close, slow_period)
        self.d_update(f_line, self.d_value(f_fast) - self.d_value(f_slow))
        self.d_update_ema(f_sig, f_line, sig_period)
        self.d_update(f_hist, self.d_value(f_line) - self.d_value(f_sig))

    def d_update_sto(self, k_period :int, d_period :int, s_period :int):
        key = f'sto({k_period},{d_period},{s_period})'
        f_k = self.field_index[key + ':k']
        f_d = self.field_index[key + ':d']
        f_j = self.field_index[key + ':j']
        min_low = self.d_min(self.field_index['low'], k_period)
        max_high = self.d_max(self.field_index['high'], k_period)
        self.d_update(f_k, 100 * (self.d_value(self.field_index['high']) - min_low) / (max_high - min_low))
        self.d_update(f_d, self.d_ma(f_k, d_period))
        self.d_update(f_j, self.d_ma(f_d, s_period))

    def d_update_rsi(self, period :int):
        f_rsi = self.field_index[f'rsi({period})']
        delta = self.d_window(self.field_index['diff:close'], period)
        gain = np.where(delta > 0, delta, 0).mean() if delta.shape[0] == period else np.nan
        loss = np.where(delta < 0, -delta, 0).mean() if delta.shape[0] == period else np.nan
        self.d_update(f_rsi, 100 - (100 / (1 + gain / loss)) if loss != 0 else 100)
 
    def d_update_bol(self, period :int):
        key = f'bol({period})'
        f_u = self.field_index[key + ':u']
        f_l = self.field_index[key + ':l']
        f_close = self.field_index['close']
        ma = self.d_ma(f_close, period)
        std = self.d_std(f_close, period)
        self.d_update(f_u, ma + std * 2)
        self.d_update(f_l, ma - std * 2)
    
    def d_update_cci(self, period :int):
        f_cci = self.field_index[f'cci({period})']
        f_tp = self.field_index['tp']
        ar = self.d_window(f_tp, period)
        sma = ar.mean() if ar.shape[0] == period else np.nan
        mad = np.abs(ar - sma).mean()
        self.d_update(f_cci, (self.d_value(f_tp) - sma) / (0.015 * mad))

    def d_update_adx(self, period :int):
        # TR
        f_tr = self.field_index['tr']
        cp_prev = self.d_prev(self.field_index['close'])
        hp = self.d_value(self.field_index['high'])
        lp = self.d_value(self.field_index['low'])
        self.d_update(f_tr, np.max([hp - lp, np.abs(hp - cp_prev), np.abs(lp - cp_prev)]))
        # ATR
        f_atr = self.field_index[f'atr({period})']
        self.d_update(f_atr, self.d_ma(f_tr, period))
        atr = self.d_value(f_atr)
        # DI+
        f_di_p = self.field_index[f'di({period}):p']
        plus_dm = self.d_window(self.field_index['diff:high'], period)
        plus_dm[plus_dm < 0] = 0
        self.d_update(f_di_p, 100 * (plus_dm.mean() / atr))
        # DI-
        f_di_m = self.field_index[f'di({period}):m']
        minus_dm = self.d_window(self.field_index['diff:low'], period)
        minus_dm[minus_dm > 0] = 0
        self.d_update(f_di_m, 100 * (np.abs(minus_dm.mean()) / atr))
        # DX
        f_dx = self.field_index[f'dx({period})']
        plus_di = self.d_value(f_di_p)
        minus_di = self.d_value(f_di_m)
        self.d_update(f_dx, 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di))
        # ADX
        f_adx = self.field_index[f'adx({period})']
        self.d_update(f_adx, self.d_ma(f_dx, period))

    def d_update_disp(self, period :int):
        f_disp = self.field_index[f'disp({period})']
        f_ma = self.field_index[f'ma({period}):close']
        self.d_update(f_disp, (self.d_value(self.field_index['close']) / self.d_value(f_ma)) * 100)
        
    def d_update_psy(self, period :int):
        f_psy = self.field_index[f'psy({period})']
        ar = self.d_window(self.field_index['diff:close'], period)
        up_mins = (ar > 0).sum() if ar.shape[0] == period else np.nan
        self.d_update(f_psy, (up_mins / period) * 100)

    def d_update_obv(self, period :int):
        f_obv = self.field_index[f'obv({period})']
        f_raw = self.field_index['obv']
        f_close = self.field_index['close']
        close_diff = self.d_diff(f_close)
        if np.isnan(close_diff):
            raw = 0
        elif close_diff > 0:
            raw = self.d_prev(f_raw) + self.d_value(self.field_index['vol'])
        else:
            raw = self.d_prev(f_raw) - self.d_value(self.field_index['vol'])
        self.d_update(f_raw, raw)
        self.d_update_ema(f_obv, f_raw, period)
    
    def d_update_mfi(self, period :int):
        f_mfi = self.field_index[f'mfi({period})']
        f_tp = self.field_index['tp']
        f_mf_p = self.field_index['mf:p']
        f_mf_m = self.field_index['mf:m']
        price_change = self.d_value(f_tp) - self.d_prev(f_tp)
        money_flow = self.d_value(f_tp) * self.d_value(self.field_index['vol'])
        self.d_update(f_mf_p, money_flow if price_change > 0 else 0)
        self.d_update(f_mf_m, money_flow if price_change < 0 else 0)
        flow_mp = self.d_ma(f_mf_p, period)
        flow_mm = self.d_ma(f_mf_m, period)
        mfi = 100 - (100 / (1 + flow_mp / flow_mm)) if flow_mm != 0 else 100
        self.d_update(f_mfi, mfi)

    def d_update_vo(self, short_period :int, long_period :int):
        f_vo = self.field_index[f'vo({short_period})']
        f_vol = self.field_index['vol']
        short_vma = self.d_ma(f_vol, short_period)
        long_vma = self.d_ma(f_vol, long_period)
        self.d_update(f_vo, (short_vma - long_vma) / long_vma * 100)

    def d_update_vr(self, period :int):
        f_vr = self.field_index[f'vr({period})']
        f_v_p = self.field_index['vol:p']
        f_v_z = self.field_index['vol:z']
        f_v_m = self.field_index['vol:m']
        f_close = self.field_index['close']
        f_vol = self.field_index['vol']
        price_change = self.d_value(f_close) - self.d_prev(f_close)
        vol = self.d_value(f_vol)
        self.d_update(f_v_p, vol if price_change > 0 else 0)
        self.d_update(f_v_z, vol if price_change == 0 else 0)
        self.d_update(f_v_m, vol if price_change < 0 else 0)
        if np.isnan(price_change):
            vr = np.nan
        else:
            vol_p = self.d_window(f_v_p, period).sum()
            vol_z = self.d_window(f_v_z, period).sum()
            vol_m = self.d_window(f_v_m, period).sum()
            tmp = vol_m + vol_z * .5
            vr = (vol_p + vol_z * .5) / tmp * 100 if tmp != 0 else 999999
        self.d_update(f_vr, vr)

    def d_update_sar(self, step: float, max_step: float):
        key = f'sar({step},{max_step})'
        f_sar = self.field_index[key]
        f_trend = self.field_index[key + ':trend']
        f_ep = self.field_index[key + ':ep']
        f_af = self.field_index[key + ':af']
        f_close = self.field_index['close']
        f_high = self.field_index['high']
        f_low = self.field_index['low']
        high = self.d_value(f_high)
        low = self.d_value(f_low)
        if np.isnan(self.d_prev(f_close)):
            trend = 1
            sar = high if trend < 0 else low
            ep = high if trend > 0 else low
            af = step
        else:
            sar = self.d_prev(f_sar) + self.d_prev(f_af) * (self.d_prev(f_ep) - self.d_prev(f_sar))
            trend_prev = self.d_prev(f_trend)
            af_prev = self.d_prev(f_af)
            ep_prev = self.d_prev(f_ep)
            low_prev = self.d_prev(f_low)
            low_prev2 = self.d_prev(f_low, 2)
            if np.isnan(low_prev2):
                low_prev2 = low_prev
            high_prev = self.d_prev(f_high)
            high_prev2 = self.d_prev(f_high, 2)
            if np.isnan(high_prev2):
                high_prev2 = high_prev
            if self.d_prev(f_trend) > 0:
                sar = min(sar, low_prev, low_prev2)
                if high > ep_prev:
                    ep = high
                    af = min(af_prev + step, max_step)
                else:
                    ep = ep_prev
                    af = af_prev
            else:
                sar = max(sar, high_prev, high_prev2)
                if low < ep_prev:
                    ep = low
                    af = min(af_prev + step, max_step)
                else:
                    ep = ep_prev
                    af = af_prev
            if (trend_prev > 0 and low < sar) or (trend_prev < 0 and high > sar):
                trend = -trend_prev
                sar = ep_prev
                ep = high if trend > 0 else low
                af = step
            else:
                trend = trend_prev
        self.d_update(f_trend, trend)
        self.d_update(f_ep, ep)
        self.d_update(f_af, af)
        self.d_update(f_sar, sar)

    def d_update_trix(self, period :int):
        key = f'trix({period})'
        f_trix = self.field_index[key]
        f_close = self.field_index['close']
        f_ema1 = self.field_index[f'ema({period}):close']
        f_ema2 = self.field_index[key + ':2']
        f_ema3 = self.field_index[key + ':3']
        self.d_update_ema(f_ema1, f_close, period)
        self.d_update_ema(f_ema2, f_ema1, period)
        self.d_update_ema(f_ema3, f_ema2, period)
        ema3_prev = self.d_prev(f_ema3)
        trix = (self.d_value(f_ema3) - ema3_prev) / ema3_prev * 100
        self.d_update(f_trix, trix)
    
    def d_update_sonar(self, n_period :int, k_period :int, m_period :int):
        key = f'sonar({n_period},{k_period},{m_period})'
        f_sonar = self.field_index[key]
        f_signal = self.field_index[key + ':sig']
        f_close = self.field_index['close']
        f_ema = self.field_index[f'ema({n_period}):close']
        self.d_update_ema(f_ema, f_close, n_period)
        self.d_update(f_sonar, self.d_value(f_ema) - self.d_prev(f_ema, k_period))
        self.d_update_ema(f_signal, f_sonar, m_period)


    # static message handler list
    @staticmethod
    def message_handler_field_names_get(self, args: tuple):
        self.print_log('field_names.get')
        self.socket.send((True, self.field_name[self.field_visible]))

    @staticmethod
    def message_handler_csv_last_dt(self, args: tuple):
        self.print_log('csv.last_dt')
        self.socket.send(self.get_csv_last_datetime())

    @staticmethod
    def message_handler_csv_first_dt(self, args: tuple):
        self.print_log('csv.first_dt')
        self.socket.send(self.get_csv_first_datetime())

    @staticmethod
    def message_handler_csv_append(self, args: tuple):
        self.print_log(f'csv.append: {args}')
        # datetime, open, high, low, close, vol
        if len(args) > 5:
            self.socket.send(self.append_csv(*args[:6]))
        else:
            self.socket.send((False, '(datetime, open, high, low, close, vol) needed'))

    @staticmethod
    def message_handler_csv_get(self, args: tuple):
        self.print_log(f'csv.get: {args}')
        if len(args) > 0:
            self.socket.send(self.get_csv(args[0]))
        else:
            self.socket.send((False, '(date, ) needed'))

    @staticmethod
    def message_handler_data_train(self, args: tuple):
        self.print_log(f'data.train: {args}')
        # in:  (input_size, list_of_ahead, start_date, end_date)
        # out: (bool, X, Y)
        if len(args) > 1:
            input_size = args[0]
            aheads = args[1]
            start_date = args[2] if len(args) > 2 else 0
            end_date = args[3] if len(args) > 3 else 0
            X, Y = self.get_train_data(input_size, aheads, start_date, end_date)
            self.socket.send((True, X, Y))
        else:
            self.socket.send((False, '(input_size, list_of_ahead[, start_date[, end_date]]) needed'))

    @staticmethod
    def message_handler_data_test(self, args: tuple):
        self.print_log(f'data.test: {args}')
        # in:  (input_size, start_date, end_date)
        # out: (bool, list of tXy)
        if len(args) > 0:
            input_size = int(args[0])
            start_date = args[1] if len(args) > 1 else 0
            end_date = args[2] if len(args) > 2 else 0
            list_of_tXy = self.get_test_data(input_size, start_date, end_date)
            self.socket.send((True, list_of_tXy))
        else:
            self.socket.send((False, '(input_size[, start_date[, end_date]]) needed'))

    @staticmethod
    def message_handler_data_get(self, args: tuple):
        self.print_log(f'data.get: {args}')
        # in:  (input_size, datetime)
        # out: (bool, X, y)
        if len(args) > 1:
            input_size = args[0]
            datetime = args[1]
            X, y = self.get_data(input_size, datetime)
            self.socket.send((True, X, y))
        else:
            self.socket.send((False, '(input_size, datetime) needed'))

    @staticmethod
    def message_handler_data_rebuild(self, args: tuple):
        self.print_log('data.rebuild')
        self.socket.send(self.rebuild_data())

    @staticmethod
    def message_handler_data_normalize(self, args: tuple):
        self.print_log(f'data.dnormalize: {args}')
        # in:  ([start_date, end_date])
        # out: (bool, )
        start_date, end_date = args[:2] if len(args) > 1 else (0, 0)
        X, _ = self.get_train_data(1, start_date=start_date, end_date=end_date)
        offset = X.mean(axis=0)
        self.offset = np.expand_dims(offset, axis=0)
        self.config['offset'] = offset.tolist()
        X -= self.offset
        scale = X.std(axis=0)
        self.config['scale'] = scale.tolist()
        self.scale = np.expand_dims(scale, axis=0)
        self.config_dirty = True
        self.save_config()
        self.socket.send((True, ))

    @staticmethod
    def message_handler_model_exist(self, args: tuple):
        self.print_log(f'model.exist: {args}')
        # in:  (som_m, som_n, input_size, aheads)
        # out: (bool, )
        if len(args) > 3:
            som_m = args[0]
            som_n = args[1]
            input_size = args[2]
            aheads = args[3]
            fn = f'{self.dataset}/model/som_{som_m}x{som_n}x{input_size}.pkl'
            if os.path.exists(fn):
                with open(fn, 'rb') as f:
                    som, dict_of_group_stats = pickle.load(f)
                if all(key in dict_of_group_stats for key in aheads):
                    self.socket.send((True, ))
                else:
                    self.socket.send((False, 'model(ahead) not found'))
            else:
                self.socket.send((False, 'model(file) not found'))
        else:
            self.socket.send((False, '(input_size, som_m, som_n, ahead) needed'))

    @staticmethod
    def message_handler_model_get(self, args):
        self.print_log(f'model.get: {args}')
        # in:  (som_m, som_n, input_size, ahead)
        # out: (bool, som, group_stats)
        if len(args) > 3:
            som_m = args[0]
            som_n = args[1]
            input_size = args[2]
            ahead = args[3]
            fn = f'{self.dataset}/model/som_{som_m}x{som_n}x{input_size}.pkl'
            if os.path.exists(fn):
                with open(fn, 'rb') as f:
                    som, dict_of_group_stats = pickle.load(f)
                if ahead in dict_of_group_stats:
                    self.socket.send((True, som, dict_of_group_stats[ahead]))
                else:
                    self.socket.send((False, 'model(ahead) not found'))
            else:
                self.socket.send((False, 'model(file) not found'))
        else:
            self.socket.send((False, '(input_size, som_m, som_n, ahead) needed'))

    @staticmethod
    def message_handler_model_set(self, args):
        self.print_log(f'model.set: {args[:3]}')
        # in:  (som_m, som_n, input_size, som. group_stats)
        # out: (bool, )
        if len(args) > 4:
            som_m = args[0]
            som_n = args[1]
            input_size = args[2]
            som = args[3]
            dict_of_group_stats = args[4]
            path = f'{self.dataset}/model'
            if not os.path.exists(path):
                os.mkdir(path)
            fn = f'{path}/som_{som_m}x{som_n}x{input_size}.pkl'
            with open(fn, 'wb') as f:
                pickle.dump((som, dict_of_group_stats), f)
            self.socket.send((True, ))
        else:
            self.socket.send((False, '(input_size, som_m, som_n, ahead, som, group_stats) needed'))

    @staticmethod
    def message_handler_ping(self, args):
        self.print_log('ping')
        self.socket.send((True, ))

    @staticmethod
    def message_handler_save(self, args):
        self.print_log('save')
        self.save()
        self.socket.send((True, ))

    @staticmethod
    def message_handler_exit(self, args):
        self.print_log('exit')
        self.save()
        self.exit = True

    @staticmethod
    def message_handler_quit(self, args):
        self.print_log('quit')
        self.exit = True

    def init_message_handler(self):
        self.message_handler = {
            'field_names': {
                # out: (bool, field_names)
                'get': data_service_server.message_handler_field_names_get
            },
            'csv': {
                # out: (bool, last_datetime)
                'last_dt': data_service_server.message_handler_csv_last_dt,
                # out: (bool, first_datetime)
                'first_dt': data_service_server.message_handler_csv_first_dt,
                # in:  (datetime, open, high, low, close, vol)
                # out: (bool, )
                'append': data_service_server.message_handler_csv_append,
                # in: (date. )
                # out: (bool, list of csv)
                'get': data_service_server.message_handler_csv_get
            },
            'data': {
                # in:  (input_size, list_of_ahead, start_date, end_date)
                # out: (bool, X, Y)
                'train': data_service_server.message_handler_data_train,
                # in:  (input_size, start_date, end_date)
                # out: (bool, list of tXy)
                'test': data_service_server.message_handler_data_test,
                # in:  (input_size, datetime)
                # out: (bool, X, y)
                'get': data_service_server.message_handler_data_get,
                # out: (bool, )
                'rebuild': data_service_server.message_handler_data_rebuild,
                # in:  ([start_date, end_date])
                # out: (bool, )
                'normalize': data_service_server.message_handler_data_normalize
            },
            'model': {
                # in:  (som_m, som_n, input_size, aheads)
                # out: (bool, )
                'exist': data_service_server.message_handler_model_exist,
                # in:  (som_m, som_n, input_size, ahead)
                # out: (bool, som, group_stat)
                'get': data_service_server.message_handler_model_get,
                # in:  (som_m, som_n, input_size, som, group_stats)
                # out: (bool, )
                'set': data_service_server.message_handler_model_set
            },
            # out: (bool, )
            'ping': data_service_server.message_handler_ping,
            'save': data_service_server.message_handler_save,
            'exit': data_service_server.message_handler_exit,
            'quit': data_service_server.message_handler_quit
        }


class data_service_client:
    def __init__(self, dataset):
        self.socket = edge_socket().connect('data_service', dataset)

    def field_names_get(self):
        self.socket.send(('field_names', 'get'))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 2:
            print('field_names.get.error:', resp)
            return None
        return resp[1]

    def csv_last_dt(self):
        self.socket.send(('csv', 'last_dt'))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 2:
            print('csv.last_dt.error:', resp)
            return None
        return resp[1]

    def csv_first_dt(self):
        self.socket.send(('csv', 'first_dt'))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 2:
            print('csv.first_dt.error:', resp)
            return None
        return resp[1]

    def csv_append(self, datetime, open, high, low, close, vol):
        self.socket.send(('csv', 'append', datetime, open, high, low, close, vol))
        resp = self.socket.recv()
        if resp[0] == False:
            print('csv.append.error:', resp)
        return resp[0]

    def data_train(self, input_size, aheads, start_date=0, end_date=0):
        if isinstance(aheads, int):
            aheads = [aheads]
        self.socket.send(('data', 'train', input_size, aheads, start_date, end_date))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 3:
            print('data.train.error:', resp)
            return None
        return resp[1], resp[2]

    def data_test(self, input_size, start_date=0, end_date=0):
        self.socket.send(('data', 'test', input_size, start_date, end_date))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 2:
            print('data.test.error:', resp)
            return None
        return resp[1]

    def data_get(self, input_size, datetime):
        self.socket.send(('data', 'get', input_size, datetime))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 3:
            print('data.get.error:', resp)
            return None
        return resp[1], resp[2]

    def data_rebuild(self):
        self.socket.send(('data', 'rebuild'))
        resp = self.socket.recv()
        if resp[0] == False:
            print('data.rebuild.error:', resp)
        return resp[0]

    def data_normalize(self, start_date=0, end_date=0):
        self.socket.send(('data', 'normalize', start_date, end_date))
        resp = self.socket.recv()
        if resp[0] == False:
            print('data.normalize.error:', resp)
        return resp[0]

    def model_exist(self, som_m, som_n, input_size, aheads):
        if isinstance(aheads, int):
            aheads = [aheads]
        self.socket.send(('model', 'exist', som_m, som_n, input_size, aheads))
        resp = self.socket.recv()
        return resp[0]

    def model_get(self, som_m, som_n, input_size, ahead):
        self.socket.send(('model', 'get', som_m, som_n, input_size, ahead))
        resp = self.socket.recv()
        if resp[0] == False or len(resp) != 3:
            print('model.get.error:', resp)
            return None
        return resp[1], resp[2]

    def model_set(self, som_m, som_n, input_size, som, group_stats):
        self.socket.send(('model', 'set', som_m, som_n, input_size, som, group_stats))
        resp = self.socket.recv()
        if resp[0] == False:
            print('model.set.error:', resp)
        return resp[0]

    def ping(self):
        self.socket.send(('ping', ))
        resp = self.socket.recv()
        if resp[0] == False:
            print('save.error:', resp)
        return resp[0]

    def save(self):
        self.socket.send(('save', ))
        resp = self.socket.recv()
        if resp[0] == False:
            print('save.error:', resp)
        return resp[0]

    def exit(self):
        self.socket.send(('exit', ))

    def quit(self):
        self.socket.send(('quit', ))



if __name__ == "__main__":
    if len(sys.argv) < 2:
        print('usage: python data_service.py dataset')
    else:
        ds = data_service_server(sys.argv[1])
        ds.server()
